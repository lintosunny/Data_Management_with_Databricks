# Data Management with Databricks: Big Data with Delta Lakes

## Overview
This project is based on the Coursera guided project "Data Management with Databricks: Big Data with Delta Lakes." The project introduces Delta Lakes and demonstrates how to use Databricks to manage big data workloads, ensuring data consistency and optimizing performance. The course covers Delta Lake architecture, schema evolution, and version control, culminating in the creation of a Supply Chain dashboard.

## Learning Objectives
* **Create Delta Tables in Databricks:** Set up and manage Delta Tables, a robust data storage format optimized for performance and reliability.
* **Data Transformation and Querying:** Use Python to transform Delta Tables and SQL to query data, ultimately creating visualizations for a Supply Chain dashboard.
* **Delta Lake Merge and Version Control:** Utilize Delta Lake's merge operation for efficient data updates and leverage version control to track and revert to previous versions of Delta Tables.

## Getting Started
1. Set up a Databricks environment
   [Databricks Community Account](https://community.cloud.databricks.com/login.html?tuuid=747349d2-f837-4fb2-a8c2-5440d869f3bf)
2. Open New notebook
3. Import dataset/Coursera GP_Data Management with Databricks_ Big Data with Delta Lakes_learners Guide.ipynb to notebook
4. Enable DBFS web in settings
5. Upload data to Catalog > DBFS > FileStore > SupplyChain

## Tools Used
Python, SQL, Databricks, Delta Lake
